{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where I am experimenting with how to call and download the data from datahub's Covid-19 database\n",
    "\n",
    "**The location of the data site is https://datahub.io/core/covid-19#data**\n",
    "\n",
    "**The files are in CSV, JSON and ZIP file formats**\n",
    "\n",
    "US data is located at /core/covid-19/r/us_confirmed.cvs (or .json)\n",
    "US data is located at /core/covid-19/r/us_deaths.cvs (or .json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 # BeautifulSoup an HTML parser\n",
    "import os # library for saving the files within my systm\n",
    "import re # regular expression library for finding data within the data\n",
    "import requests # Downloads data from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the file name that I save to my machine from the website so I'm not puling requests all the time\n",
    "file_name = \"covid-19-data.html\" \n",
    "# website of the data\n",
    "base_url = \"https://datahub.io/core/covid-19#data\" \n",
    "#Can try to make an OOP to incorperate these two variables into it for doing the rest of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to search the datahub website to find and download the latest data.\n",
    "## This can of course be addressed manually but for practice this code would \n",
    "## help me search for the link to download the data\n",
    "\n",
    "## This can of course be addressed manually but for practice this code would \n",
    "## help me search for the link to download the data\n",
    "\n",
    "if os.path.isfile(file_name): # may wish to delete this after the code has run or compare dates\n",
    "    print(f\"Using locally cached data from {file_name}\") \n",
    "    with open(file_name, encoding='utf-8') as file:\n",
    "        html_text = file.read()\n",
    "else:\n",
    "    print(f\"Loading data from {base_url}\")\n",
    "    req = requests.get(base_url)\n",
    "    req.raise_for_status()\n",
    "    html_text = req.text\n",
    "    with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "        file.write(html_text)\n",
    "res = bs4.BeautifulSoup(html_text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying out the idea of making a class to do the web scraping for me.**\n",
    "\n",
    "Trying to make tools not single use code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would the class look like to turn this into a tool instead of a single use code?\n",
    "class WebScrape:\n",
    "    def __init__(self,file_name,base_url):\n",
    "        self.name = name\n",
    "        self.file_name = file_name\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def Load:\n",
    "        if os.path.isfile(file_name): # may wish to delete this after the code has run or compare dates\n",
    "        print(f\"Using locally cached data from {file_name}\") \n",
    "        with open(file_name, encoding='utf-8') as file:\n",
    "            html_text = file.read()\n",
    "        else:\n",
    "            print(f\"Loading data from {base_url}\")\n",
    "            req = requests.get(base_url)\n",
    "            req.raise_for_status()\n",
    "            html_text = req.text\n",
    "            with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "                file.write(html_text)\n",
    "        res = bs4.BeautifulSoup(html_text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(res.select('.download > a'))# selects \"a href\" tags underneath .downlads with the CSS selectors\n",
    "links #This does take specific knowlege on how this particular website is setup and not code to be used generally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay now I have the locations of the files... now what?  \n",
    "# First I have to pull out the address from the rest of the files under the <a /a> heading\n",
    "File_links= list(re.findall(r'\"([^\"]*)\"', str(links)))\n",
    "File_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next is do I set something that lets one choose what to download?  .csv? .json? .zip?\n",
    "while True:\n",
    "    File_Type= input ('What File Type do you wish to download? (.csv, .json, .zip)' )\n",
    "    if '.csv' in File_Type or '.json' in File_Type or '.zip' in File_Type:\n",
    "        print ('File type accepted')\n",
    "        break\n",
    "    else:\n",
    "        print (\"Wrong File Type!\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then I need to add back in the full address of the file location.\n",
    "addresses=[]\n",
    "for Type in File_links:\n",
    "    if File_Type in Type:\n",
    "        print ('http://datahub.io'+Type)\n",
    "        addresses.append ('http://datahub.io'+Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an alternate way to do this.\n",
    "addresses=[]\n",
    "for Type in File_links:\n",
    "    if Type.endswith(File_Type):\n",
    "        print ('http://datahub.io'+Type)\n",
    "        addresses.append ('http://datahub.io'+Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get just the file name so I can save the file?\n",
    "File_name=[]\n",
    "for Type in addresses:\n",
    "    File_name.append(os.path.basename(Type)) #os.path gets either the path or the file name (aka basename)\n",
    "File_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for File in addresses:\n",
    "    response = requests.get(File)# downloads the csv file\n",
    "    print(f\"Downloading {File}\")  # Updates us as to the status of the file (I could have it count down)\n",
    "    with open(os.path.join(os.path.basename(File)), 'wb') as f: # creates a file to write to\n",
    "        f.write(response.content) #writes the data to the file\n",
    "print ('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay what would the class look like which is searching for my filetype and downloading it?\n",
    "class Download_File:\n",
    "    __init__(self,File_Type):\n",
    "        self=self.self\n",
    "        self.File_Type=File_Type\n",
    "        self.File_name=File_name\n",
    "        self.address=address \n",
    "# User inputs the filetype to search for\n",
    "        while True: \n",
    "            File_Type= input ('What File Type do you wish to download? (.csv, .json, .zip)' ) # Should I have it scan for the file types available?\n",
    "            if '.csv' in File_Type or '.json' in File_Type or '.zip' in File_Type:\n",
    "                print ('File type accepted')\n",
    "                break\n",
    "            else:\n",
    "                print (\"Wrong File Type! Try again.\")\n",
    "                continue\n",
    "#File locations are pulled off the webpage                \n",
    "        addresses=[]\n",
    "        for Type in File_links:\n",
    "            if Type.endswith(File_Type):\n",
    "                print ('http://datahub.io'+Type)\n",
    "                addresses.append ('http://datahub.io'+Type)\n",
    "#The File name is pulled from the address to save the file locally to                \n",
    "        File_name=[]\n",
    "        for Type in addresses:\n",
    "            File_name.append(os.path.basename(Type)) #os.path gets either the path or the file name (aka basename)\n",
    "        for File in addresses:\n",
    "            response = requests.get(File)# downloads the csv file\n",
    "            print(f\"Downloading {File}\")  # Updates us as to the status of the file (I could have it count down)\n",
    "            with open(os.path.join(os.path.basename(File)), 'wb') as f: # creates a file to write to\n",
    "                f.write(response.content) #writes the data to the file\n",
    "        print ('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That is one way... Here is another**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to get the files is to look for all a tags with href and a .csv\n",
    "quotes = []\n",
    "for quote in res.select(\"a[href$='.csv']\"):\n",
    "    quotes.append(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set (to remove duplicates) of the text within the \" \"\n",
    "Files_quotes = set (re.findall(r'\"([^\"]*)\"', str(quotes))) \n",
    "Files_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method tries to select for css codes which start with a href and end with .csv \n",
    "# It's a more general approach to finding the file locations and doesn't use a for loop\n",
    "if File_Type =='.csv':\n",
    "    links_href=res.select(\"a[href$='.csv']\")\n",
    "elif File_Type =='.json':\n",
    "    links_href=res.select(\"a[href$='.json']\")\n",
    "else:\n",
    "    links_href=res.select(\"a[href$='.zip']\")\n",
    "links_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set (to remove duplicates) of the text within the \" \"\n",
    "Files_href = set (re.findall(r'\"([^\"]*)\"', str(links_href))) \n",
    "Files_href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That grabs some things which are not .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this alteration will get the addresses of all the files knowing the starting address\n",
    "Files_csv = set (re.findall(r'\"/core/covid-19/r/([^\"]*)\"', str(links_href))) \n",
    "Files_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method works too, but I have to add back in a lot of the file path by hand\n",
    "for address in Files_csv:\n",
    "    File_path = 'https://datahub.io/core/covid-19/r/'+address\n",
    "    print (File_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This bit of code works to download the file to my computer to use.\n",
    "#The nice thing about this way is I can use the csv file name as the file name too\n",
    "for address in Files_csv:\n",
    "    File_path = 'https://datahub.io/core/covid-19/r/'+ address\n",
    "    response = requests.get(File_path)# downloads the csv file\n",
    "    with open(os.path.join(address), 'wb') as f: # creates a file to write to\n",
    "        f.write(response.content) #writes the data to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step is to start combing through the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
