{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I am experimenting with how to call and download the data from datahub's Covid-19 database\n",
    "\n",
    "The location of the data site is https://datahub.io/core/covid-19#data\n",
    "The files are in both JSON and CSV file format\n",
    "\n",
    "US data is located at /core/covid-19/r/us_confirmed.cvs (or .json)\n",
    "US data is located at /core/covid-19/r/us_deaths.cvs (or .json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"covid-19-data.html\"\n",
    "base_url = \"https://datahub.io/core/covid-19#data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to search the datahub website to find and download the latest data.\n",
    "## This can of course be addressed manually but for practice this code would \n",
    "## help me search for the link to download the data\n",
    "\n",
    "## This can of course be addressed manually but for practice this code would \n",
    "## help me search for the link to download the data\n",
    "if os.path.isfile(file_name):\n",
    "    print(f\"Using locally cached data from {file_name}\")\n",
    "    with open(file_name, encoding='utf-8') as file:\n",
    "        html_text = file.read()\n",
    "else:\n",
    "    print(f\"Loading data from {base_url}\")\n",
    "    req = requests.get(base_url)\n",
    "    req.raise_for_status()\n",
    "    html_text = req.text\n",
    "    with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "        file.write(html_text)\n",
    "res = bs4.BeautifulSoup(html_text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = res.select('.download > a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.select(\"a[href$='.csv']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links =res.select(\"a[href$='.csv']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links # Can I make this a string where I split the data inbetween the \"\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url):\n",
    "    with requests.Session() as req:\n",
    "        r = req.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        target = [f\"{url[:20]}{item['href']}\" for item in soup.select(\n",
    "            \"a[href$='.csv']\")]\n",
    "        for x in target:\n",
    "            print(f\"Downloading {x}\")\n",
    "            r = req.get(x)\n",
    "            name = x.rsplit(\"/\", 1)[-1]\n",
    "            with open(name, 'wb') as f:\n",
    "                f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_str=str(links)\n",
    "type(links_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def multi_re_find(patterns,phrase):\n",
    "    '''\n",
    "    Takes in a list of regex patterns\n",
    "    Prints a list of all matches\n",
    "    '''\n",
    "    for pattern in patterns:\n",
    "        print('Searching the phrase using the re check: %r' %(pattern))\n",
    "        print(re.findall(pattern,phrase))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern =['\\w+.csv','/core/covid-19/r/[\\w+]+.csv','[\\W+]+[\\w+]+[\\W+]+[\\w+]+.csv','\\w+']\n",
    "multi_re_find(pattern,str(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=set(re.findall('[^/core[\\W+\\w+]{0,10}.csv]',links_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_location=set(re.findall('[^/core[\\W+\\w+]{10,50}.csv]+',links_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This bit of code works to download the file to my computer to use.\n",
    "response = requests.get('https://datahub.io/core/covid-19/r/us_simplified.csv')# downloads the csv file\n",
    "with open(os.path.join('simplified.csv'), 'wb') as f: # creates a file to write to\n",
    "    f.write(response.content) #writes the data to the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
