{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@ProjectName: DXY-2019-nCov-Crawler\n",
    "@FileName: crawler.py\n",
    "@Author: Jiabao Lin\n",
    "@Date: 2020/1/21\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from service.db import DB\n",
    "from service.userAgent import user_agent_list\n",
    "from service.nameMap import country_type_map, city_name_map, country_name_map, continent_name_map\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self):\n",
    "        self.session = requests.session()\n",
    "        self.db = DB()\n",
    "        self.crawl_timestamp = int()\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            self.crawler()\n",
    "            time.sleep(60)\n",
    "\n",
    "    def crawler(self):\n",
    "        while True:\n",
    "            self.session.headers.update(\n",
    "                {\n",
    "                    'user-agent': random.choice(user_agent_list)\n",
    "                }\n",
    "            )\n",
    "            self.crawl_timestamp = int(time.time() * 1000)\n",
    "            try:\n",
    "                r = self.session.get(url='https://ncov.dxy.cn/ncovh5/view/pneumonia')\n",
    "            except requests.exceptions.ChunkedEncodingError:\n",
    "                continue\n",
    "            soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "            overall_information = re.search(r'(\\{\"id\".*\\})\\}', str(soup.find('script', attrs={'id': 'getStatisticsService'})))\n",
    "            if overall_information:\n",
    "                self.overall_parser(overall_information=overall_information)\n",
    "\n",
    "            area_information = re.search(r'\\[(.*)\\]', str(soup.find('script', attrs={'id': 'getAreaStat'})))\n",
    "            if area_information:\n",
    "                self.area_parser(area_information=area_information)\n",
    "\n",
    "            abroad_information = re.search(r'\\[(.*)\\]', str(soup.find('script', attrs={'id': 'getListByCountryTypeService2true'})))\n",
    "            if abroad_information:\n",
    "                self.abroad_parser(abroad_information=abroad_information)\n",
    "\n",
    "            news_chinese = re.search(r'\\[(.*?)\\]', str(soup.find('script', attrs={'id': 'getTimelineService1'})))\n",
    "            if news_chinese:\n",
    "                self.news_parser(news=news_chinese)\n",
    "\n",
    "            news_english = re.search(r'\\[(.*?)\\]', str(soup.find('script', attrs={'id': 'getTimelineService2'})))\n",
    "            if news_english:\n",
    "                self.news_parser(news=news_english)\n",
    "\n",
    "            rumors = re.search(r'\\[(.*?)\\]', str(soup.find('script', attrs={'id': 'getIndexRumorList'})))\n",
    "            if rumors:\n",
    "                self.rumor_parser(rumors=rumors)\n",
    "\n",
    "            if not overall_information or \\\n",
    "                    not area_information or \\\n",
    "                    not abroad_information or \\\n",
    "                    not news_chinese or \\\n",
    "                    not news_english or \\\n",
    "                    not rumors:\n",
    "                time.sleep(3)\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        logger.info('Successfully crawled.')\n",
    "\n",
    "    def overall_parser(self, overall_information):\n",
    "        overall_information = json.loads(overall_information.group(1))\n",
    "        overall_information.pop('id')\n",
    "        overall_information.pop('createTime')\n",
    "        overall_information.pop('modifyTime')\n",
    "        overall_information.pop('imgUrl')\n",
    "        overall_information.pop('deleted')\n",
    "        overall_information['countRemark'] = overall_information['countRemark'].replace(' 疑似', '，疑似').replace(' 治愈', '，治愈').replace(' 死亡', '，死亡').replace(' ', '')\n",
    "\n",
    "        if not self.db.find_one(collection='DXYOverall', data=overall_information):\n",
    "            overall_information['updateTime'] = self.crawl_timestamp\n",
    "\n",
    "            self.db.insert(collection='DXYOverall', data=overall_information)\n",
    "\n",
    "    def province_parser(self, province_information):\n",
    "        provinces = json.loads(province_information.group(0))\n",
    "        for province in provinces:\n",
    "            province.pop('id')\n",
    "            province.pop('tags')\n",
    "            province.pop('sort')\n",
    "            province['comment'] = province['comment'].replace(' ', '')\n",
    "\n",
    "            if self.db.find_one(collection='DXYProvince', data=province):\n",
    "                continue\n",
    "\n",
    "            province['provinceEnglishName'] = city_name_map[province['provinceShortName']]['engName']\n",
    "            province['crawlTime'] = self.crawl_timestamp\n",
    "            province['country'] = country_type_map.get(province['countryType'])\n",
    "\n",
    "            self.db.insert(collection='DXYProvince', data=province)\n",
    "\n",
    "    def area_parser(self, area_information):\n",
    "        area_information = json.loads(area_information.group(0))\n",
    "        for area in area_information:\n",
    "            area['comment'] = area['comment'].replace(' ', '')\n",
    "\n",
    "            # Because the cities are given other attributes,\n",
    "            # this part should not be used when checking the identical document.\n",
    "            cities_backup = area.pop('cities')\n",
    "\n",
    "            if self.db.find_one(collection='DXYArea', data=area):\n",
    "                continue\n",
    "\n",
    "            # If this document is not in current database, insert this attribute back to the document.\n",
    "            area['cities'] = cities_backup\n",
    "\n",
    "            area['countryName'] = '中国'\n",
    "            area['countryEnglishName'] = 'China'\n",
    "            area['continentName'] = '亚洲'\n",
    "            area['continentEnglishName'] = 'Asia'\n",
    "            area['provinceEnglishName'] = city_name_map[area['provinceShortName']]['engName']\n",
    "\n",
    "            for city in area['cities']:\n",
    "                if city['cityName'] != '待明确地区':\n",
    "                    try:\n",
    "                        city['cityEnglishName'] = city_name_map[area['provinceShortName']]['cities'][city['cityName']]\n",
    "                    except KeyError:\n",
    "                        print(area['provinceShortName'], city['cityName'])\n",
    "                        pass\n",
    "                else:\n",
    "                    city['cityEnglishName'] = 'Area not defined'\n",
    "\n",
    "            area['updateTime'] = self.crawl_timestamp\n",
    "\n",
    "            self.db.insert(collection='DXYArea', data=area)\n",
    "\n",
    "    def abroad_parser(self, abroad_information):\n",
    "        countries = json.loads(abroad_information.group(0))\n",
    "        for country in countries:\n",
    "            try:\n",
    "                country.pop('id')\n",
    "                country.pop('tags')\n",
    "                country.pop('sort')\n",
    "                # Ding Xiang Yuan have a large number of duplicates,\n",
    "                # values are all the same, but the modifyTime are different.\n",
    "                # I suppose the modifyTime is modification time for all documents, other than for only this document.\n",
    "                # So this field will be popped out.\n",
    "                country.pop('modifyTime')\n",
    "                # createTime is also different even if the values are same.\n",
    "                # Originally, the createTime represent the first diagnosis of the virus in this area,\n",
    "                # but it seems different for abroad information.\n",
    "                country.pop('createTime')\n",
    "                country['comment'] = country['comment'].replace(' ', '')\n",
    "            except KeyError:\n",
    "                pass\n",
    "            country.pop('countryType')\n",
    "            country.pop('provinceId')\n",
    "            country.pop('cityName')\n",
    "            # The original provinceShortName are blank string\n",
    "            country.pop('provinceShortName')\n",
    "            # Rename the key continents to continentName\n",
    "            country['continentName'] = country.pop('continents')\n",
    "\n",
    "            if self.db.find_one(collection='DXYArea', data=country):\n",
    "                continue\n",
    "\n",
    "            country['countryName'] = country.get('provinceName')\n",
    "            country['provinceShortName'] = country.get('provinceName')\n",
    "            country['continentEnglishName'] = continent_name_map.get(country['continentName'])\n",
    "            country['countryEnglishName'] = country_name_map.get(country['countryName'])\n",
    "            country['provinceEnglishName'] = country_name_map.get(country['countryName'])\n",
    "\n",
    "            country['updateTime'] = self.crawl_timestamp\n",
    "\n",
    "            self.db.insert(collection='DXYArea', data=country)\n",
    "\n",
    "    def news_parser(self, news):\n",
    "        news = json.loads(news.group(0))\n",
    "        for _news in news:\n",
    "            _news.pop('pubDateStr')\n",
    "            if self.db.find_one(collection='DXYNews', data=_news):\n",
    "                continue\n",
    "            _news['crawlTime'] = self.crawl_timestamp\n",
    "\n",
    "            self.db.insert(collection='DXYNews', data=_news)\n",
    "\n",
    "    def rumor_parser(self, rumors):\n",
    "        rumors = json.loads(rumors.group(0))\n",
    "        for rumor in rumors:\n",
    "            rumor.pop('score')\n",
    "            rumor['body'] = rumor['body'].replace(' ', '')\n",
    "            if self.db.find_one(collection='DXYRumors', data=rumor):\n",
    "                continue\n",
    "            rumor['crawlTime'] = self.crawl_timestamp\n",
    "\n",
    "            self.db.insert(collection='DXYRumors', data=rumor)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler = Crawler()\n",
    "    crawler.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
